# ERNIE Desktop Documentation

## 1. Overview

ERNIE Desktop is a self-contained local LLM assistant that couples an offline inference stack (llama.cpp), a FastAPI service that augments responses with live web search and hardware telemetry, and a lightweight browser UI. Everything ships together so you can launch a full-featured chat experience from a single script with no external cloud dependencies.

### Key Capabilities

- Local inference using `llama.cpp` binaries and bundled `e4.gguf` model.
- FastAPI microservice that proxies Tavily search and streams host power/RAM/CPU/temperature metrics.
- Rich web UI with session management, attachment ingestion (text/PDF/DOCX/CSV/code), Markdown rendering, syntax highlighting, and search context injection.
- **Conversation branching** with message editing, response regeneration, and tree navigation for exploring alternative conversation paths.
- Single launcher (`ed.sh`) that writes runtime config, boots both backends, and opens the UI in a fullscreen Chromium window.

## 2. Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     JSON completions     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Web UI      â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚ llama.cpp  â”‚
â”‚ (ernie.html) â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚  server    â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚  REST (fetch)                               â–²
      â”‚                                             â”‚
      â–¼                                             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” Tavily + telemetry  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ FastAPI      â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚ Tavily API /
â”‚ search.py    â”‚                    â”‚ Host sensors
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Components

- `chat/`: llama.cpp binaries + GGUF model, launched in server mode.
- `search/search.py`: FastAPI app providing `/search/web`, `/telemetry/power`, and `/health`.
- `ernie.html` + `runtime-config.js`: Single-page UI served from local filesystem.
- `lib/`: Pinned copies of Bootstrap, Marked, Highlight.js, and pdf.js for offline use.
- `ed.sh`: Launcher orchestrating env loading, runtime config generation, process startup, and UI opening.

## 3. Repository Layout

```
ED/
â”œâ”€â”€ chat/                 # llama.cpp executables and shared libs
â”œâ”€â”€ search/               # FastAPI app + virtualenv (if desired)
â”œâ”€â”€ lib/                  # Front-end third-party JS/CSS
â”œâ”€â”€ ernie.html            # UI
â”œâ”€â”€ runtime-config.js     # Generated at launch
â”œâ”€â”€ ed.sh                 # Launcher
â”œâ”€â”€ .env                  # Primary configuration file
â”œâ”€â”€ docs/                 # Documentation assets
â””â”€â”€ *.log                 # Runtime logs (llama.log, search.log)
```

## 4. Requirements

- 64-bit Linux host (tested on Radxa Orion O6 ARM SBC).
- `chromium` in `$PATH` (for fullscreen UI).
- Python 3.11+ with `psutil`, `fastapi`, `uvicorn`, `tavily`, and dependencies installed (use the provided virtualenv or system packages).
- Tavily API key for web search (`TAVILY_API_KEY` in `.env`).
- Adequate RAM/VRAM for the bundled model (â‰ˆ6â€¯GB recommended).

## 5. Configuration

All runtime knobs live in `.env`. Notable fields:

- `LLM_HOST`, `LLM_PORT`, `LLM_MODEL_PATH`: llama.cpp server binding + GGUF path (absolute or relative to `chat/`).
- `API_HOST`, `API_PORT`, `API_MODEL`: FastAPI binding and Tavily model label.
- `TAVILY_API_KEY`: Required for `/search/web`.
- `WEB_POLL_TIMER_MS`, `WEB_TELEMETRY_MS`: UI polling cadence for server health and telemetry.
- `PYTHON_ENV_PATH`: Optional venv to auto-activate before launching services.
- `SEARCH_VENV_PATH`: Preferred interpreter for `search.py` (falls back to `PYTHON_ENV_PATH` or `python3`).
- `POWER_IDLE_WATTS`, `POWER_MAX_WATTS`: Used to normalize power utilization/indicator colors.
- `UI_BROWSER_CMD`: Override command used to open the UI (defaults to Chromium fullscreen fallback logic).

## 6. Launcher Workflow (`ed.sh`)

1. Loads `.env`, exporting values for downstream processes.
2. Activates `$PYTHON_ENV_PATH` when provided.
3. Writes `runtime-config.js` with resolved endpoint URLs and poll intervals.
4. Starts `llama-server` (7 threads, 2,048 ctx, ubatch=4, `--mlock`) and tails logs to `llama.log`.
5. Locates a Python interpreter (prefers `$SEARCH_VENV_PATH` / `$PYTHON_ENV_PATH`) and runs `search.py`, logging to `search.log`.
6. Opens `ernie.html` in Chromium (fullscreen `--app` window). Falls back to Firefox kiosk or `xdg-open`.
7. Traps `Ctrl+C` to cleanly terminate both processes.

### Useful Commands

```bash
# Run everything
./ed.sh

# Inspect logs
tail -f llama.log
tail -f search.log
```

## 7. FastAPI Search & Telemetry Service

- `GET /`: Service metadata and endpoint list.
- `GET /health`: Returns `{"status": "healthy"}` when alive.
- `POST /search/web`: Body `{"query": "...", "count": 5}`. Uses Tavily, formats results for UI consumption, returns a `SearchResponse`.
- `GET /telemetry/power`: Returns `PowerTelemetry` with watts, battery status, RAM usage, CPU usage, CPU temp, and derived color thresholds.

### Telemetry Sources

- **Power**: `/sys/class/power_supply/*` first, then `/sys/class/hwmon`, finally CPU-utilization-based estimate.
- **RAM**: `psutil.virtual_memory()`.
- **CPU usage**: `psutil.cpu_percent(interval=0.05)`.
- **Temperature**: `psutil.sensors_temperatures()` with preference order `coretemp`, `k10temp`, `cpu-thermal`, etc.
- **Battery Plug State**: `psutil.sensors_battery()` when available.

## 8. Web UI Highlights

- **Session Management**: Start new chats, persist to `localStorage`, import/export sessions as JSON with full conversation tree structure.
- **Messaging Workflow**: Type, `Ctrl+Enter` to send, or use buttons. Attach multiple files; text/code/CSV/DOCX is inlined, PDFs parsed via pdf.js, binary files referenced.
- **Conversation Branching**: Edit user messages, regenerate assistant responses, and navigate between alternative conversation paths using branch controls.
- **Search Integration**: `Search` button runs Tavily query, stores context for the next user message, and clears the input automatically.
- **Markdown Rendering**: Powered by Marked + Highlight.js with light/dark themes. Each code block includes copy/download actions.
- **Performance Metrics**: TTFT, generation time, characters, tokens, and session tokens/sec displayed live.
- **Telemetry Header**: Power, RAM, CPU, and Temp pills update every `WEB_TELEMETRY_MS`. Color bands indicate severity (green/amber/red) with tooltips for timestamps.
- **Theme & Settings**: Temperature/top-k/p sampling options, stop sequences, manual token caps, and persistent theme toggle.
- **File Storage**: Local `lib/` libraries (Bootstrap, Marked, Highlight.js, pdf.js, PapaParse, mammoth.js) guarantee offline functionality and faster loads.

## 9. Telemetry Color Logic

- **Power utilization**: green `<60%` of idleâ€“max span (or `<25â€¯W` when span unknown); yellow `60â€“85%` (`25â€“40â€¯W`); red `â‰¥85%` (`â‰¥40â€¯W`).
- **RAM usage**: green `<70%`; yellow `70â€“85%`; red `â‰¥85%`.
- **CPU usage**: green `<60%`; yellow `60â€“85%`; red `â‰¥85%`.
- **CPU temperature**: green `<60â€¯Â°C`; yellow `60â€“75â€¯Â°C`; red `â‰¥75â€¯Â°C`.

Idle/default state shows grey pills while the UI is initializing or after a session reset.

## 10. Conversation Branching

ERNIE Desktop features a conversation tree system that allows you to explore alternative responses and edit messages without losing your conversation history.

### How It Works

Instead of a simple linear chat history, conversations are stored as a tree where each message can have multiple child responses. This enables:

- **Editing user messages** and regenerating responses
- **Creating alternative assistant responses** at any point
- **Navigating between branches** to compare different conversation paths
- **Preserving all responses** so you never lose an interesting answer

### Tree Structure Example

```
User: "Explain quantum physics"
â”œâ”€â–º Assistant: "Here's a technical explanation..." (Branch 1)
â”œâ”€â–º Assistant: "In simple terms..." (Branch 2)
â””â”€â–º Assistant: "Let me use an analogy..." (Branch 3)
```

Each branch represents a different response to the same question. You can switch between them without losing any content.

### Editing Messages

When you edit a user message, ERNIE will:

1. Prompt you to confirm (since this deletes all responses after that message)
2. Update the message content
3. Automatically generate a new response to the edited message
4. Preserve the conversation history up to that point

**Use Case**: Refine your question after seeing the initial response.

**Example**:
- Original: "Explain quantum physics" â†’ Response is too technical
- Edit to: "Explain quantum physics simply" â†’ Get a simplified response

### Regenerating Responses

Click the ğŸ”„ **Regenerate** button on any assistant message to create an alternative response. Each regeneration:

- Creates a new "sibling" response to the same user message
- Keeps all previous responses accessible
- Doesn't affect the rest of the conversation

**Use Case**: Get multiple perspectives or writing styles for the same question.

**Example**:
- Ask: "Write a haiku about coding"
- Regenerate 3 times to get 4 different haikus
- Choose your favorite using the branch navigator

### Branch Navigation

When a message has multiple children (branches), you'll see navigation controls:

```
â—€  2/4  â–¶
```

- **â—€ Button**: Switch to previous sibling response
- **Counter**: Shows current branch (2) of total branches (4)
- **â–¶ Button**: Switch to next sibling response
- Buttons are disabled at the boundaries

The entire conversation updates to follow the selected branch path.

### Practical Workflows

#### Workflow 1: Refining Questions
1. Ask a question
2. Review the response
3. Click âœï¸ **Edit** to refine your question
4. Get a better-targeted response

#### Workflow 2: Exploring Options
1. Ask an open-ended question
2. Click ğŸ”„ **Regenerate** multiple times
3. Use â—€â–¶ to browse all responses
4. Continue from your preferred response

#### Workflow 3: Conversation Branches
1. Have a conversation about Topic A
2. Get to an interesting point
3. Regenerate to explore Topic B instead
4. Now you have two conversation paths from the same starting point

### Session Management with Branches

- **Saving**: All branches are preserved when you save a session
- **Loading**: Sessions restore with the full tree structure intact
- **Exporting**: Export format (v2.0) includes complete tree data
- **Importing**: Supports both old linear (v1.0) and new tree (v2.0) formats
- **Legacy Support**: Old session files are automatically converted to tree format

## 11. File Attachments & Context Building

1. User text is combined with (optional) search context and attachment summaries.
2. Text/code files under ~200â€¯KB are embedded directly; larger or binary files are acknowledged but not inlined.
3. PDF ingestion extracts text per page via pdf.js; embedded text is appended to the prompt builder.
4. The session retains pending search contexts until the next send, allowing multiple search invocations before sending.

## 11. Troubleshooting

- **UI pills show `n/a`**: FastAPI unreachable or psutil lacks sensors. Check `search.log`, hit `/telemetry/power`, fix permissions.
- **Search calls fail with 503**: `TAVILY_API_KEY` missing or invalid. Update `.env`, rerun launcher.
- **llama-server exits immediately**: Model path wrong or insufficient RAM. Verify `LLM_MODEL_PATH`, free memory, adjust `--threads`.
- **Chromium fails to launch**: Binary absent or custom command misconfigured. Install Chromium or fix `UI_BROWSER_CMD`.
- **PDF attachments not parsed**: pdf.js assets missing. Confirm `lib/pdf.min.js` and `lib/pdf.worker.min.js`.

## 12. Performance Tips

- Match `--threads` to physical cores; adjust `--batch-size`/`--ubatch-size` for your CPU.
- Enable `--flash-attn` or quantized models if your llama.cpp build supports them for ARM.
- Reduce `WEB_TELEMETRY_MS` only if needed; lower intervals increase psutil sampling overhead.
- Use smaller context windows when latency is more important than maximum history.

## 13. Security Considerations

- The UI loads from `file://` and fetches only localhost endpoints by default.
- Tavily calls are proxied server-side to avoid exposing API keys to the browser.
- Uploaded files never leave the browser; only extracted text is sent to the model.
- Logs may contain prompts; rotate or redact as needed during demos.

## 14. Third-Party Licenses

- **Bootstrap 5.3.3** â€” MIT License.
- **Marked 11.x** â€” MIT License.
- **Highlight.js 11.9.0** â€” BSD-3-Clause License.
- **pdf.js 3.11.174** â€” Apache-2.0 License.
- **Tavily Python SDK** â€” MIT License.

Include license notices if redistributing binaries or UI assets.

## 15. Generating This Document

The Markdown source (`docs/ernie-desktop-guide.md`) can be converted to HTML/PDF locally:

```bash
# HTML (uses helper script)
python3 tools/md_to_html.py docs/ernie-desktop-guide.md docs/ernie-desktop-guide.html

# PDF (requires chromium)
chromium --headless --disable-gpu \
  --print-to-pdf=docs/ernie-desktop-guide.pdf \
  docs/ernie-desktop-guide.html
```

The repository already includes a pre-generated PDF; regenerate after major changes for accuracy.

## 16. Change Log (excerpt)

- **Latest**: Offline JS libs, environment-driven Python activation, telemetry badges with CPU/temp, PDF export instructions.
- See git history for earlier iterations.

---

Happy hacking with ERNIE Desktop!
